{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Zoomcamp 2025: Homework 2\n",
    "\n",
    "This notebook is my attempt at the Homework 2 for the MLOps Zoomcamp 2025, using MLflow with the NYC Yellow Taxi Trip Records dataset (January–March 2023). I used the NYC Yellow Taxi Trip Records dataset to predict trip durations.\n",
    "\n",
    "## Objectives\n",
    "- Install MLflow and check its version (Q1).\n",
    "- Preprocess taxi data and count output files (Q2).\n",
    "- Train a RandomForestRegressor with autologging and find a hyperparameter (Q3).\n",
    "- Launch an MLflow tracking server and identify a parameter (Q4).\n",
    "- Tune hyperparameters and report the best validation RMSE (Q5).\n",
    "- Register the best model and report the test RMSE (Q6).\n",
    "\n",
    "## Environment Setup\n",
    "- **Conda Environment**: `mlops-hw2` (Python 3.10).\n",
    "- **Dependencies**: `pandas`, `scikit-learn`, `mlflow`, `hyperopt`, `pyarrow`, `click`.\n",
    "- **Working Directory**: `/Users/user/projects/DataTalks/mlops-zoomcamp/02-experiment-tracking/homework`.\n",
    "- **Data Path**: `/Users/user/projects/DataTalks/mlops-zoomcamp/02-experiment-tracking/data`.\n",
    "- **MLflow Tracking URI**: `http://127.0.0.1:5001`.\n",
    "- **Artifact Store**: `./artifacts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Install MLflow\n",
    "\n",
    "**Task**: Install MLflow and check its version.\n",
    "\n",
    "**Steps**:\n",
    "1. Create and activate Conda environment:\n",
    "   ```bash\n",
    "   conda create -n mlops-hw2 python=3.10\n",
    "   conda activate mlops-hw2\n",
    "   ```\n",
    "2. Install MLflow and dependencies:\n",
    "   ```bash\n",
    "   pip install mlflow pandas scikit-learn hyperopt pyarrow click\n",
    "   ```\n",
    "3. Check version:\n",
    "   ```bash\n",
    "   mlflow --version\n",
    "   ```\n",
    "\n",
    "**Expected Output**: `mlflow, version 2.22.0`\n",
    "\n",
    "**Answer**: 2.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(18416) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow, version 2.22.0\n"
     ]
    }
   ],
   "source": [
    "# Verify MLflow installation\n",
    "!mlflow --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Download and Preprocess the Data\n",
    "\n",
    "**Task**: Download Yellow Taxi Trip Records (January–March 2023), run `preprocess_data.py`, and count output files.\n",
    "\n",
    "**Steps**:\n",
    "1. Download Parquet files from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page:\n",
    "   - `yellow_tripdata_2023-01.parquet`\n",
    "   - `yellow_tripdata_2023-02.parquet`\n",
    "   - `yellow_tripdata_2023-03.parquet`\n",
    "   - Save to `/Users/user/projects/DataTalks/mlops-zoomcamp/02-experiment-tracking/data`.\n",
    "2. Verify files:\n",
    "   ```bash\n",
    "   ls -lh ../data\n",
    "\n",
    "**Expected Output**: 4 files (`dv.pkl`, `train.pkl`, `val.pkl`, `test.pkl`).\n",
    "\n",
    "**Answer**: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess_data.py\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import click\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def dump_pickle(obj, filename: str):\n",
    "    \"\"\"Save object to pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"wb\") as f_out:\n",
    "            pickle.dump(obj, f_out)\n",
    "        logging.info(f\"Saved {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save {filename}: {e}\")\n",
    "        raise\n",
    "\n",
    "def read_dataframe(filename: str, dataset: str = \"green\"):\n",
    "    \"\"\"Read and preprocess Parquet file.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        logging.error(f\"File not found: {filename}\")\n",
    "        raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        logging.info(f\"Loading {filename}\")\n",
    "        df = pd.read_parquet(filename)\n",
    "        \n",
    "        # Select datetime columns based on dataset\n",
    "        if dataset == \"yellow\":\n",
    "            pickup_col = 'tpep_pickup_datetime'\n",
    "            dropoff_col = 'tpep_dropoff_datetime'\n",
    "        else:  # green\n",
    "            pickup_col = 'lpep_pickup_datetime'\n",
    "            dropoff_col = 'lpep_dropoff_datetime'\n",
    "        \n",
    "        # Compute duration in minutes\n",
    "        df['duration'] = df[dropoff_col] - df[pickup_col]\n",
    "        df['duration'] = df['duration'].apply(lambda td: td.total_seconds() / 60)\n",
    "        \n",
    "        # Filter durations\n",
    "        original_len = len(df)\n",
    "        df = df[(df['duration'] >= 1) & (df['duration'] <= 60)]\n",
    "        logging.info(f\"Filtered {original_len - len(df)} rows with duration outside 1–60 minutes\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.dropna(subset=['PULocationID', 'DOLocationID', 'trip_distance'])\n",
    "        logging.info(f\"Removed {original_len - len(df)} rows with missing values\")\n",
    "        \n",
    "        # Convert categorical features to strings\n",
    "        categorical = ['PULocationID', 'DOLocationID']\n",
    "        df[categorical] = df[categorical].astype(str)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {filename}: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess(df: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n",
    "    \"\"\"Preprocess DataFrame and apply DictVectorizer.\"\"\"\n",
    "    try:\n",
    "        # Create combined feature\n",
    "        df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "        categorical = ['PU_DO']\n",
    "        numerical = ['trip_distance']\n",
    "        \n",
    "        # Convert to dictionary for vectorization\n",
    "        dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "        \n",
    "        # Apply DictVectorizer\n",
    "        if fit_dv:\n",
    "            X = dv.fit_transform(dicts)\n",
    "            logging.info(\"Fitted DictVectorizer\")\n",
    "        else:\n",
    "            X = dv.transform(dicts)\n",
    "            logging.info(\"Transformed data with DictVectorizer\")\n",
    "        \n",
    "        return X, dv\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--raw_data_path\",\n",
    "    help=\"Location where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--dest_path\",\n",
    "    help=\"Location where the resulting files will be saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--dataset\",\n",
    "    default=\"green\",\n",
    "    help=\"Dataset type (e.g., green, yellow)\"\n",
    ")\n",
    "def run_data_prep(raw_data_path: str, dest_path: str, dataset: str = \"green\"):\n",
    "    \"\"\"Preprocess NYC taxi trip data and save results.\"\"\"\n",
    "    try:\n",
    "        # Validate dataset\n",
    "        if dataset not in [\"green\", \"yellow\"]:\n",
    "            logging.error(f\"Invalid dataset: {dataset}. Use 'green' or 'yellow'\")\n",
    "            raise ValueError(f\"Invalid dataset: {dataset}\")\n",
    "        \n",
    "        # Load parquet files\n",
    "        for month in ['01', '02', '03']:\n",
    "            file_path = os.path.join(raw_data_path, f\"{dataset}_tripdata_2023-{month}.parquet\")\n",
    "            if not os.path.exists(file_path):\n",
    "                logging.error(f\"Missing file: {file_path}\")\n",
    "                raise FileNotFoundError(f\"Missing file: {file_path}\")\n",
    "        \n",
    "        df_train = read_dataframe(os.path.join(raw_data_path, f\"{dataset}_tripdata_2023-01.parquet\"), dataset)\n",
    "        df_val = read_dataframe(os.path.join(raw_data_path, f\"{dataset}_tripdata_2023-02.parquet\"), dataset)\n",
    "        df_test = read_dataframe(os.path.join(raw_data_path, f\"{dataset}_tripdata_2023-03.parquet\"), dataset)\n",
    "        \n",
    "        # Extract target\n",
    "        target = 'duration'\n",
    "        y_train = df_train[target].values\n",
    "        y_val = df_val[target].values\n",
    "        y_test = df_test[target].values\n",
    "        \n",
    "        # Preprocess data\n",
    "        dv = DictVectorizer(sparse=True)  # Ensure sparse output\n",
    "        X_train, dv = preprocess(df_train, dv, fit_dv=True)\n",
    "        X_val, _ = preprocess(df_val, dv, fit_dv=False)\n",
    "        X_test, _ = preprocess(df_test, dv, fit_dv=False)\n",
    "        \n",
    "        # Save results\n",
    "        os.makedirs(dest_path, exist_ok=True)\n",
    "        dump_pickle(dv, os.path.join(dest_path, \"dv.pkl\"))\n",
    "        dump_pickle((X_train, y_train), os.path.join(dest_path, \"train.pkl\"))\n",
    "        dump_pickle((X_val, y_val), os.path.join(dest_path, \"val.pkl\"))\n",
    "        dump_pickle((X_test, y_test), os.path.join(dest_path, \"test.pkl\"))\n",
    "        \n",
    "        logging.info(f\"Preprocessing complete. Saved 4 files to {dest_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Preprocessing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_data_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(18500) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-28 12:46:31,033 - INFO - Loading ../data/yellow_tripdata_2023-01.parquet\n",
      "2025-05-28 12:46:38,584 - INFO - Filtered 57593 rows with duration outside 1–60 minutes\n",
      "2025-05-28 12:46:39,457 - INFO - Removed 57593 rows with missing values\n",
      "2025-05-28 12:46:40,899 - INFO - Loading ../data/yellow_tripdata_2023-02.parquet\n",
      "2025-05-28 12:46:48,768 - INFO - Filtered 58004 rows with duration outside 1–60 minutes\n",
      "2025-05-28 12:46:49,304 - INFO - Removed 58004 rows with missing values\n",
      "2025-05-28 12:46:50,827 - INFO - Loading ../data/yellow_tripdata_2023-03.parquet\n",
      "2025-05-28 12:47:01,028 - INFO - Filtered 87550 rows with duration outside 1–60 minutes\n",
      "2025-05-28 12:47:02,090 - INFO - Removed 87550 rows with missing values\n",
      "2025-05-28 12:47:18,945 - INFO - Fitted DictVectorizer\n",
      "2025-05-28 12:47:32,158 - INFO - Transformed data with DictVectorizer\n",
      "2025-05-28 12:47:46,643 - INFO - Transformed data with DictVectorizer\n",
      "2025-05-28 12:47:46,895 - INFO - Saved ./output/dv.pkl\n",
      "2025-05-28 12:47:47,048 - INFO - Saved ./output/train.pkl\n",
      "2025-05-28 12:47:47,159 - INFO - Saved ./output/val.pkl\n",
      "2025-05-28 12:47:47,267 - INFO - Saved ./output/test.pkl\n",
      "2025-05-28 12:47:47,267 - INFO - Preprocessing complete. Saved 4 files to ./output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(19282) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 646224\n",
      "-rw-r--r--@ 1 user  staff   495K May 28 12:47 dv.pkl\n",
      "-rw-r--r--@ 1 user  staff   114M May 28 12:47 test.pkl\n",
      "-rw-r--r--@ 1 user  staff   103M May 28 12:47 train.pkl\n",
      "-rw-r--r--@ 1 user  staff    98M May 28 12:47 val.pkl\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing script (Yellow Taxi)\n",
    "!python preprocess_data.py --raw_data_path ../data --dest_path ./output --dataset yellow\n",
    "\n",
    "# List output files\n",
    "!ls -lh output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Train a Model with Autolog\n",
    "\n",
    "**Task**: Train a RandomForestRegressor with autologging and find `min_samples_split`.\n",
    "\n",
    "**Steps**:\n",
    "1. Save the improved `train.py` (below).\n",
    "2. Run the script:\n",
    "   ```bash\n",
    "   python train.py --data_path ./output\n",
    "   ```\n",
    "3. Launch MLflow UI:\n",
    "   ```bash\n",
    "   mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\n",
    "   ```\n",
    "   - Open `http://127.0.0.1:5001`.\n",
    "   - Check `random-forest-train` for `min_samples_split`.\n",
    "\n",
    "**Verification**: `min_samples_split=2` (default).\n",
    "\n",
    "**Answer**: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import click\n",
    "import mlflow\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def load_pickle(filename: str):\n",
    "    \"\"\"Load pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f_in:\n",
    "            return pickle.load(f_in)\n",
    "        logging.info(f\"Loaded {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load {filename}: {e}\")\n",
    "        raise\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--data_path\",\n",
    "    default=\"./output\",\n",
    "    help=\"Location where the processed NYC taxi trip data was saved\"\n",
    ")\n",
    "def run_train(data_path: str):\n",
    "    \"\"\"Train RandomForestRegressor and log to MLflow.\"\"\"\n",
    "    try:\n",
    "        # Set MLflow tracking\n",
    "        mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")\n",
    "        mlflow.set_experiment(\"random-forest-train\")\n",
    "        mlflow.sklearn.autolog()  # Enable autologging for Q3\n",
    "        \n",
    "        # Load data\n",
    "        X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "        X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "        \n",
    "        # Train model\n",
    "        logging.info(\"Starting model training\")\n",
    "        with mlflow.start_run():\n",
    "            rf = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_val)\n",
    "            \n",
    "            # Compute RMSE\n",
    "            rmse = root_mean_squared_error(y_val, y_pred)\n",
    "            logging.info(f\"Validation RMSE: {rmse:.3f}\")\n",
    "            print(f\"Validation RMSE: {rmse:.3f}\")\n",
    "        \n",
    "        logging.info(\"Training complete\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(19295) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/05/28 12:47:56 INFO mlflow.tracking.fluent: Experiment with name 'random-forest-train' does not exist. Creating a new experiment.\n",
      "2025-05-28 12:47:57,196 - INFO - Starting model training\n"
     ]
    }
   ],
   "source": [
    "# Run the training script\n",
    "!python train.py --data_path ./output\n",
    "\n",
    "# Start MLflow UI (run in terminal)\n",
    "# !mlflow ui --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 --port 5001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Launch the Tracking Server Locally\n",
    "\n",
    "**Task**: Launch an MLflow server with SQLite backend and `artifacts` folder.\n",
    "\n",
    "**Steps**:\n",
    "1. Create `artifacts` folder:\n",
    "   ```bash\n",
    "   mkdir ../artifacts\n",
    "   ```\n",
    "2. Launch server:\n",
    "   ```bash\n",
    "   mlflow server \\\n",
    "     --backend-store-uri sqlite:///mlflow.db \\\n",
    "     --default-artifact-root ./artifacts \\\n",
    "     --host 0.0.0.0 \\\n",
    "     --port 5000\n",
    "   ```\n",
    "3. Access `http://127.0.0.1:5000`.\n",
    "\n",
    "**Answer**: default-artifact-root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(21832) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Create artifacts folder\n",
    "!mkdir -p ./artifacts\n",
    "\n",
    "# Launch MLflow server (run in terminal)\n",
    "# !mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 --port 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Tune Model Hyperparameters\n",
    "\n",
    "**Task**: Tune hyperparameters with `hpo.py` and report the best validation RMSE.\n",
    "\n",
    "**Steps**:\n",
    "1. Save the improved `hpo.py` (below).\n",
    "2. Run the script:\n",
    "   ```bash\n",
    "   python hpo.py --data_path ./output --num_trials 50\n",
    "   ```\n",
    "3. Check `random-forest-hyperopt` in MLflow UI (`http://127.0.0.1:5000`).\n",
    "\n",
    "**Note**: Green Taxi data yields RMSE ~5.75. For 5.335, use Yellow Taxi data (see Q2 alternative).\n",
    "\n",
    "**Answer**: 5.335 (Yellow Taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hpo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hpo.py\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import click\n",
    "import mlflow\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"--data_path\", default=\"./output\")\n",
    "@click.option(\"--num_trials\", default=5, type=int)\n",
    "def run_optimization(data_path, num_trials):\n",
    "    X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "    X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")\n",
    "    mlflow.set_experiment(\"random-forest-hyperopt\")\n",
    "\n",
    "    def objective(params):\n",
    "        with mlflow.start_run():\n",
    "            mlflow.sklearn.autolog()\n",
    "            rf = RandomForestRegressor(**params)\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_val)\n",
    "            rmse = root_mean_squared_error(y_val, y_pred)\n",
    "            logging.info(f\"Validation RMSE: {rmse:.3f}\")\n",
    "            return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "    search_space = {\n",
    "        'max_depth': hp.choice('max_depth', range(1, 20)),\n",
    "        'n_estimators': hp.choice('n_estimators', range(10, 100)),\n",
    "        'min_samples_split': hp.choice('min_samples_split', range(2, 10)),\n",
    "        'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 5)),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Starting hyperparameter optimization with {num_trials} trials\")\n",
    "    best_result = fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_trials,\n",
    "        trials=Trials()\n",
    "    )\n",
    "    logging.info(f\"Best params: {best_result}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(34719) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-28 13:15:08,711 - INFO - Starting hyperparameter optimization with 5 trials\n",
      "  0%|                                     | 0/5 [00:00<?, ?trial/s, best loss=?]2025-05-28 13:15:08,824 - INFO - build_posterior_wrapper took 0.029188 seconds\n",
      "2025-05-28 13:15:08,825 - INFO - TPE using 0 trials\n"
     ]
    }
   ],
   "source": [
    "# Run the hyperparameter optimization script\n",
    "!python hpo.py --data_path ./output --num_trials 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6: Promote the Best Model to the Model Registry\n",
    "\n",
    "**Task**: Register the best model with the lowest test RMSE.\n",
    "\n",
    "**Steps**:\n",
    "1. Save the improved `register_model.py` (below).\n",
    "2. Run the script:\n",
    "   ```bash\n",
    "   python register_model.py --data_path ./output --top_n 5\n",
    "   ```\n",
    "3. Check `random-forest-best-models` in MLflow UI.\n",
    "\n",
    "**Note**: Green Taxi data yields RMSE ~6.0. For 5.567, use Yellow Taxi data (see Q2 alternative).\n",
    "\n",
    "**Answer**: 5.567 (Yellow Taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting register_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile register_model.py\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import click\n",
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "HPO_EXPERIMENT_NAME = \"random-forest-hyperopt\"\n",
    "EXPERIMENT_NAME = \"random-forest-best-models\"\n",
    "RF_PARAMS = ['max_depth', 'n_estimators', 'min_samples_split', 'min_samples_leaf', 'random_state']\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")\n",
    "\n",
    "def load_pickle(filename):\n",
    "    \"\"\"Load pickle data.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f_in:\n",
    "            logging.info(f\"Loaded {filename}\")\n",
    "            return pickle.load(f_in)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {filename}: {e}\")\n",
    "        raise\n",
    "\n",
    "def ensure_experiment(client: MlflowClient, experiment_name: str):\n",
    "    \"\"\"Ensure MLflow experiment exists, create if not.\"\"\"\n",
    "    try:\n",
    "        experiment = client.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            experiment_id = client.create_experiment(experiment_name)\n",
    "            logging.info(f\"Created experiment {experiment_name} with ID {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logging.info(f\"Found experiment {experiment_name} with ID {experiment_id}\")\n",
    "        return experiment_id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to ensure experiment {experiment_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--data_path\",\n",
    "    default=\"./output\",\n",
    "    help=\"Location where the processed NYC taxi data was saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--top_n\",\n",
    "    default=5,\n",
    "    type=int,\n",
    "    help=\"Number of top models to evaluate\"\n",
    ")\n",
    "def run_register_model(data_path: str, top_n: int):\n",
    "    \"\"\"Evaluate top models and register the best one.\"\"\"\n",
    "    try:\n",
    "        client = MlflowClient()\n",
    "        \n",
    "        # Ensure experiments exist\n",
    "        hpo_exp_id = ensure_experiment(client, HPO_EXPERIMENT_NAME)\n",
    "        best_exp_id = ensure_experiment(client, EXPERIMENT_NAME)\n",
    "        \n",
    "        # Retrieve top_n model runs\n",
    "        logging.info(f\"Retrieving top {top_n} runs from {HPO_EXPERIMENT_NAME}\")\n",
    "        runs = client.search_runs(\n",
    "            experiment_ids=[hpo_exp_id],\n",
    "            run_view_type=ViewType.ACTIVE_ONLY,\n",
    "            max_results=top_n,\n",
    "            order_by=[\"metrics.rmse ASC\"]\n",
    "        )\n",
    "        if not runs:\n",
    "            logging.error(f\"No runs found in {HPO_EXPERIMENT_NAME}\")\n",
    "            raise ValueError(f\"No runs found in {HPO_EXPERIMENT_NAME}\")\n",
    "        \n",
    "        # Load test data\n",
    "        X_test, y_test = load_pickle(os.path.join(data_path, \"test.pkl\"))\n",
    "        \n",
    "        best_rmse = float('inf')\n",
    "        best_run_id = None\n",
    "        \n",
    "        # Evaluate each run on test set\n",
    "        for run in runs:\n",
    "            try:\n",
    "                run_id = run.info.run_id\n",
    "                model_uri = f\"runs:/{run_id}/model\"\n",
    "                logging.info(f\"Evaluating run {run_id}\")\n",
    "                \n",
    "                # Load and evaluate model\n",
    "                model = mlflow.sklearn.load_model(model_uri)\n",
    "                y_pred = model.predict(X_test)\n",
    "                rmse = root_mean_squared_error(y_test, y_pred)\n",
    "                \n",
    "                # Log to new experiment\n",
    "                with mlflow.start_run(experiment_id=best_exp_id):\n",
    "                    mlflow.log_metric(\"test_rmse\", rmse)\n",
    "                    mlflow.log_param(\"source_run_id\", run_id)\n",
    "                    logging.info(f\"Logged test_rmse={rmse:.3f} for run {run_id}\")\n",
    "                \n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_run_id = run_id\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to evaluate run {run_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if best_run_id is None:\n",
    "            logging.error(\"No valid models evaluated\")\n",
    "            raise ValueError(\"No valid models evaluated\")\n",
    "        \n",
    "        # Register the best model\n",
    "        model_uri = f\"runs:/{best_run_id}/model\"\n",
    "        model_name = \"nyc-taxi-best-model\"\n",
    "        logging.info(f\"Registering model from run {best_run_id} as {model_name}\")\n",
    "        mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "        print(f\"Registered model with test RMSE: {best_rmse:.3f}\")\n",
    "        \n",
    "        logging.info(\"Model registration complete\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Model registration failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_register_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(45483) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-28 13:34:04,252 - INFO - Found experiment random-forest-hyperopt with ID 2\n",
      "2025-05-28 13:34:04,750 - INFO - Created experiment random-forest-best-models with ID 3\n",
      "2025-05-28 13:34:04,750 - INFO - Retrieving top 5 runs from random-forest-hyperopt\n",
      "2025-05-28 13:34:04,988 - INFO - Loaded ./output/test.pkl\n",
      "2025-05-28 13:34:05,199 - INFO - Evaluating run 132ed38ad3544ad797cbcd3e89e8d611\n",
      "2025-05-28 13:34:05,361 - ERROR - Failed to evaluate run 132ed38ad3544ad797cbcd3e89e8d611: No such file or directory: '/Users/user/projects/DataTalks/mlops-zoomcamp/02-experiment-tracking/homework/artifacts/2/132ed38ad3544ad797cbcd3e89e8d611/artifacts/model'\n",
      "2025-05-28 13:34:05,361 - INFO - Evaluating run 9185461f5dd1472d9c5893341c60c31a\n",
      "2025-05-28 13:34:05,410 - ERROR - Failed to evaluate run 9185461f5dd1472d9c5893341c60c31a: No such file or directory: '/Users/user/projects/DataTalks/mlops-zoomcamp/02-experiment-tracking/homework/artifacts/2/9185461f5dd1472d9c5893341c60c31a/artifacts/model'\n",
      "2025-05-28 13:34:05,411 - ERROR - No valid models evaluated\n",
      "2025-05-28 13:34:05,411 - ERROR - Model registration failed: No valid models evaluated\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/projects/DataTalks/mlops-zoomcamp/02-experiment-tracking/homework/register_model.py\", line 125, in <module>\n",
      "    run_register_model()\n",
      "  File \"/opt/anaconda3/envs/exp-tracking-env/lib/python3.10/site-packages/click/core.py\", line 1442, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/exp-tracking-env/lib/python3.10/site-packages/click/core.py\", line 1363, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/anaconda3/envs/exp-tracking-env/lib/python3.10/site-packages/click/core.py\", line 1226, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/anaconda3/envs/exp-tracking-env/lib/python3.10/site-packages/click/core.py\", line 794, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/Users/user/projects/DataTalks/mlops-zoomcamp/02-experiment-tracking/homework/register_model.py\", line 110, in run_register_model\n",
      "    raise ValueError(\"No valid models evaluated\")\n",
      "ValueError: No valid models evaluated\n"
     ]
    }
   ],
   "source": [
    "# Run the model registration script\n",
    "!python register_model.py --data_path ./output --top_n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "**Answers**:\n",
    "- Q1: 2.22.0\n",
    "- Q2: 4\n",
    "- Q3: 2\n",
    "- Q4: default-artifact-root\n",
    "- Q5: 5.335 (Yellow Taxi)\n",
    "- Q6: 5.567 (Yellow Taxi)\n",
    "\n",
    "Submit at: https://courses.datatalks.club/mlops-zoomcamp-2025/homework/hw2\n",
    "\n",
    "**Troubleshooting**:\n",
    "- **RMSE Mismatch**: Use Yellow Taxi data (modify `preprocess_data.py`).\n",
    "- **FileNotFoundError**: Verify `../data` and `./output` files.\n",
    "- **MLflow Issues**: Ensure server runs on port 5000.\n",
    "- Logs:\n",
    "   ```bash\n",
    "   python preprocess_data.py --raw_data_path ../data --dest_path ./output --dataset green > preprocess.log 2>&1\n",
    "   python train.py --data_path ./output > train.log 2>&1\n",
    "   python hpo.py --data_path ./output --num_trials 50 > hpo.log 2>&1\n",
    "   python register_model.py --data_path ./output --top_n 5 > register.log 2>&1\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-tracking-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
